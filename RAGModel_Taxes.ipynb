{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def download_pdfs(pdf_links, folder_name):\n",
        "    # Download each PDF\n",
        "    for pdf_url in pdf_links:\n",
        "        pdf_name = pdf_url.split(\"/\")[-1]\n",
        "        file_path = os.path.join(folder_name, pdf_name)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"âœ… Already downloaded: {pdf_name}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pdf_res = requests.get(pdf_url)\n",
        "            pdf_res.raise_for_status()\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(pdf_res.content)\n",
        "            print(f\"ğŸ“¥ Downloaded: {pdf_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to download {pdf_name}: {e}\")\n",
        "\n",
        "    # Optional: Delay to be polite\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "tcPgXtdCiurH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3TqZugbaYCf",
        "outputId": "892fa28e-13b2-4474-cfeb-58998ab4d3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Scraping page 0...\n",
            "\n",
            "ğŸ” Scraping page 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-30080ba985a5>:35: DeprecationWarning: Call to deprecated method findChild. (Replaced by find) -- Deprecated since version 3.0.0.\n",
            "  tbody = table.findChild(\"tbody\", recursive=False)\n",
            "<ipython-input-4-30080ba985a5>:36: DeprecationWarning: Call to deprecated method findChildren. (Replaced by find_all) -- Deprecated since version 3.0.0.\n",
            "  form_rows = tbody.findChildren(\"tr\", recursive=False)\n",
            "<ipython-input-4-30080ba985a5>:41: DeprecationWarning: Call to deprecated method findChild. (Replaced by find) -- Deprecated since version 3.0.0.\n",
            "  form_desc_element = form.findChild(\"td\", attrs={\"headers\": \"view-name-table-column\"})\n",
            "<ipython-input-4-30080ba985a5>:52: DeprecationWarning: Call to deprecated method findChild. (Replaced by find) -- Deprecated since version 3.0.0.\n",
            "  form_desc_element = form.findChild(\"td\", attrs={\"headers\": \"view-name-table-column\"})\n",
            "<ipython-input-4-30080ba985a5>:55: DeprecationWarning: Call to deprecated method findChild. (Replaced by find) -- Deprecated since version 3.0.0.\n",
            "  english_forms.append(form.findChild(\"td\", attrs={\"headers\": \"view-uri-table-column\"}))\n",
            "<ipython-input-4-30080ba985a5>:60: DeprecationWarning: Call to deprecated method findChild. (Replaced by find) -- Deprecated since version 3.0.0.\n",
            "  urljoin(base_url, form.findChild(\"a\")[\"href\"])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Scraping page 2...\n",
            "\n",
            "ğŸ” Scraping page 3...\n",
            "\n",
            "ğŸ” Scraping page 4...\n",
            "\n",
            "ğŸ” Scraping page 5...\n",
            "\n",
            "ğŸ” Scraping page 6...\n",
            "\n",
            "ğŸ” Scraping page 7...\n",
            "\n",
            "ğŸ” Scraping page 8...\n",
            "\n",
            "ğŸ” Scraping page 9...\n",
            "\n",
            "ğŸ” Scraping page 10...\n",
            "\n",
            "ğŸ” Scraping page 11...\n",
            "\n",
            "ğŸ” Scraping page 12...\n",
            "\n",
            "ğŸ” Scraping page 13...\n",
            "\n",
            "ğŸ” Scraping page 14...\n",
            "\n",
            "ğŸ” Scraping page 15...\n",
            "\n",
            "ğŸ” Scraping page 16...\n",
            "\n",
            "ğŸ” Scraping page 17...\n",
            "\n",
            "ğŸ” Scraping page 18...\n",
            "\n",
            "ğŸ” Scraping page 19...\n",
            "\n",
            "ğŸ” Scraping page 20...\n",
            "\n",
            "ğŸ” Scraping page 21...\n",
            "\n",
            "ğŸ” Scraping page 22...\n",
            "\n",
            "ğŸ” Scraping page 23...\n",
            "\n",
            "ğŸ” Scraping page 24...\n",
            "\n",
            "ğŸ” Scraping page 25...\n",
            "\n",
            "ğŸ” Scraping page 26...\n",
            "\n",
            "ğŸ” Scraping page 27...\n",
            "\n",
            "ğŸ” Scraping page 28...\n",
            "\n",
            "ğŸ” Scraping page 29...\n",
            "\n",
            "ğŸ” Scraping page 30...\n",
            "\n",
            "ğŸ” Scraping page 31...\n",
            "\n",
            "ğŸ” Scraping page 32...\n",
            "\n",
            "ğŸ” Scraping page 33...\n",
            "\n",
            "ğŸ” Scraping page 34...\n",
            "\n",
            "ğŸ” Scraping page 35...\n",
            "\n",
            "ğŸ” Scraping page 36...\n",
            "\n",
            "ğŸ” Scraping page 37...\n",
            "\n",
            "ğŸ” Scraping page 38...\n",
            "\n",
            "ğŸ” Scraping page 39...\n",
            "\n",
            "ğŸ” Scraping page 40...\n",
            "\n",
            "ğŸ” Scraping page 41...\n",
            "\n",
            "ğŸ” Scraping page 42...\n",
            "\n",
            "ğŸ” Scraping page 43...\n",
            "\n",
            "ğŸ” Scraping page 44...\n",
            "\n",
            "ğŸ” Scraping page 45...\n",
            "\n",
            "ğŸ” Scraping page 46...\n",
            "\n",
            "ğŸ” Scraping page 47...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "\n",
        "# Folder to save PDFs\n",
        "os.makedirs(\"irs_eng_pdfs\", exist_ok=True)\n",
        "os.makedirs(\"irs_multilang_pdfs\", exist_ok=True)\n",
        "\n",
        "base_url = \"https://www.irs.gov\"\n",
        "start_url = \"https://www.irs.gov/downloads/irs-pdf?page={}\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "for page_num in range(58):  # Pages 0 to 57\n",
        "    print(f\"\\nğŸ” Scraping page {page_num}...\")\n",
        "    url = start_url.format(page_num)\n",
        "\n",
        "    try:\n",
        "        res = requests.get(url, headers=headers)\n",
        "        res.raise_for_status()\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to fetch page {page_num}: {e}\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    #Find all elements from the table\n",
        "    table = soup.find(\"table\", attrs={\"class\": \"table table-striped tablesaw tablesaw-stack cols-4\"})\n",
        "\n",
        "    tbody = table.findChild(\"tbody\", recursive=False)\n",
        "    form_rows = tbody.findChildren(\"tr\", recursive=False)\n",
        "\n",
        "    single_language_regex = r\"\\((sp|ru|vie|zh-s|zh-t|de|ko|ht)\\)+\"\n",
        "    filtered_forms = []\n",
        "    for form in form_rows:\n",
        "            form_desc_element = form.findChild(\"td\", attrs={\"headers\": \"view-name-table-column\"})\n",
        "            form_desc = form_desc_element.contents[0]\n",
        "            if not bool(re.search(single_language_regex, form_desc)):\n",
        "                filtered_forms.append(form)\n",
        "\n",
        "    multi_language_regex = r\"\\(en-(sp|ru|vie|zh-s|zh-t|de|ko|ht)\\)+\"\n",
        "\n",
        "    english_forms = []\n",
        "    multilang_forms = []\n",
        "\n",
        "    for form in filtered_forms:\n",
        "            form_desc_element = form.findChild(\"td\", attrs={\"headers\": \"view-name-table-column\"})\n",
        "            form_desc = form_desc_element.contents[0]\n",
        "            if not bool(re.search(single_language_regex, form_desc)):\n",
        "                english_forms.append(form.findChild(\"td\", attrs={\"headers\": \"view-uri-table-column\"}))\n",
        "            else:\n",
        "                multilang_forms.append(form.findChild(\"td\", attrs={\"headers\": \"view-uri-table-column\"}))\n",
        "\n",
        "    eng_pdf_links = [\n",
        "            urljoin(base_url, form.findChild(\"a\")[\"href\"])\n",
        "            for form in english_forms\n",
        "        ]\n",
        "\n",
        "    multilang_pdf_links = [\n",
        "            urljoin(base_url, form.findChild(\"a\")[\"href\"])\n",
        "            for form in multilang_forms\n",
        "        ]\n",
        "\n",
        "download_pdfs(eng_pdf_links, \"irs_eng_pdfs\")\n",
        "download_pdfs(multilang_pdf_links, \"irs_multilang_pdfs\")\n",
        "\n",
        "print(\"\\nğŸ‰ Done downloading all PDFs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhV92siXeOLI",
        "outputId": "f0d35168-c23b-424c-9bf4-bddcab86635e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Copy folder to a new location in your Google Drive\n",
        "shutil.copytree('irs_all_pdfs', '/content/drive/MyDrive/UTD Coursework/Sem 4/AI Agents project/Tax project/irs_all_pdfs')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "07TLglXPeOys",
        "outputId": "57331a33-7fde-4969-a87a-1dcd024bd7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/UTD Coursework/Sem 4/AI Agents project/Tax project/irs_all_pdfs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Folder to save PDFs\n",
        "os.makedirs(\"irs_all_pdfs\", exist_ok=True)\n",
        "\n",
        "base_url = \"https://www.irs.gov\"\n",
        "start_url = \"https://www.irs.gov/downloads/irs-pdf?page={}\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "for page_num in range(58):  # Pages 0 to 57\n",
        "    print(f\"\\nğŸ” Scraping page {page_num}...\")\n",
        "    url = start_url.format(page_num)\n",
        "\n",
        "    try:\n",
        "        res = requests.get(url, headers=headers)\n",
        "        res.raise_for_status()\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to fetch page {page_num}: {e}\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    # Find all <a> tags with href ending in .pdf\n",
        "    links = soup.find_all(\"a\", href=True)\n",
        "\n",
        "print(links[0])"
      ],
      "metadata": {
        "id": "N8_bnV7Qem9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48aaa013-1f8f-4792-f14c-0bf96995fef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Scraping page 0...\n",
            "\n",
            "ğŸ” Scraping page 1...\n",
            "\n",
            "ğŸ” Scraping page 2...\n",
            "\n",
            "ğŸ” Scraping page 3...\n",
            "\n",
            "ğŸ” Scraping page 4...\n",
            "\n",
            "ğŸ” Scraping page 5...\n",
            "\n",
            "ğŸ” Scraping page 6...\n",
            "\n",
            "ğŸ” Scraping page 7...\n",
            "\n",
            "ğŸ” Scraping page 8...\n",
            "\n",
            "ğŸ” Scraping page 9...\n",
            "\n",
            "ğŸ” Scraping page 10...\n",
            "\n",
            "ğŸ” Scraping page 11...\n",
            "\n",
            "ğŸ” Scraping page 12...\n",
            "\n",
            "ğŸ” Scraping page 13...\n",
            "\n",
            "ğŸ” Scraping page 14...\n",
            "\n",
            "ğŸ” Scraping page 15...\n",
            "\n",
            "ğŸ” Scraping page 16...\n",
            "\n",
            "ğŸ” Scraping page 17...\n",
            "\n",
            "ğŸ” Scraping page 18...\n",
            "\n",
            "ğŸ” Scraping page 19...\n",
            "\n",
            "ğŸ” Scraping page 20...\n",
            "\n",
            "ğŸ” Scraping page 21...\n",
            "\n",
            "ğŸ” Scraping page 22...\n",
            "\n",
            "ğŸ” Scraping page 23...\n",
            "\n",
            "ğŸ” Scraping page 24...\n",
            "\n",
            "ğŸ” Scraping page 25...\n",
            "\n",
            "ğŸ” Scraping page 26...\n",
            "\n",
            "ğŸ” Scraping page 27...\n",
            "\n",
            "ğŸ” Scraping page 28...\n",
            "\n",
            "ğŸ” Scraping page 29...\n",
            "\n",
            "ğŸ” Scraping page 30...\n",
            "\n",
            "ğŸ” Scraping page 31...\n",
            "\n",
            "ğŸ” Scraping page 32...\n",
            "\n",
            "ğŸ” Scraping page 33...\n",
            "\n",
            "ğŸ” Scraping page 34...\n",
            "\n",
            "ğŸ” Scraping page 35...\n",
            "\n",
            "ğŸ” Scraping page 36...\n",
            "\n",
            "ğŸ” Scraping page 37...\n",
            "\n",
            "ğŸ” Scraping page 38...\n",
            "\n",
            "ğŸ” Scraping page 39...\n",
            "\n",
            "ğŸ” Scraping page 40...\n",
            "\n",
            "ğŸ” Scraping page 41...\n",
            "\n",
            "ğŸ” Scraping page 42...\n",
            "\n",
            "ğŸ” Scraping page 43...\n",
            "\n",
            "ğŸ” Scraping page 44...\n",
            "\n",
            "ğŸ” Scraping page 45...\n",
            "\n",
            "ğŸ” Scraping page 46...\n",
            "\n",
            "ğŸ” Scraping page 47...\n",
            "\n",
            "ğŸ” Scraping page 48...\n",
            "\n",
            "ğŸ” Scraping page 49...\n",
            "\n",
            "ğŸ” Scraping page 50...\n",
            "\n",
            "ğŸ” Scraping page 51...\n",
            "\n",
            "ğŸ” Scraping page 52...\n",
            "\n",
            "ğŸ” Scraping page 53...\n",
            "\n",
            "ğŸ” Scraping page 54...\n",
            "\n",
            "ğŸ” Scraping page 55...\n",
            "\n",
            "ğŸ” Scraping page 56...\n",
            "\n",
            "ğŸ” Scraping page 57...\n",
            "<a class=\"visually-hidden focusable skip-link\" href=\"#main-content\">\n",
            "      Skip to main content\n",
            "    </a>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eJAeQDe5OJJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}