# -*- coding: utf-8 -*-
"""RAGModel_Taxes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vwrc4licH3xjuUQDpFJY0-WOZ49s0hVh
"""

import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Folder to save PDFs
os.makedirs("irs_all_pdfs", exist_ok=True)

base_url = "https://www.irs.gov"
start_url = "https://www.irs.gov/downloads/irs-pdf?page={}"

headers = {
    "User-Agent": "Mozilla/5.0"
}

for page_num in range(58):  # Pages 0 to 57
    print(f"\nüîç Scraping page {page_num}...")
    url = start_url.format(page_num)

    try:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to fetch page {page_num}: {e}")
        continue

    soup = BeautifulSoup(res.text, "html.parser")

    # Find all <a> tags with href ending in .pdf
    links = soup.find_all("a", href=True)
    pdf_links = [
        urljoin(base_url, a["href"])
        for a in links if a["href"].lower().endswith(".pdf")
    ]

    # Download each PDF
    for pdf_url in pdf_links:
        pdf_name = pdf_url.split("/")[-1]
        file_path = os.path.join("irs_all_pdfs", pdf_name)

        if os.path.exists(file_path):
            print(f"‚úÖ Already downloaded: {pdf_name}")
            continue

        try:
            pdf_res = requests.get(pdf_url)
            pdf_res.raise_for_status()
            with open(file_path, "wb") as f:
                f.write(pdf_res.content)
            print(f"üì• Downloaded: {pdf_name}")
        except Exception as e:
            print(f"‚ùå Failed to download {pdf_name}: {e}")

    # Optional: Delay to be polite
    time.sleep(1)

print("\nüéâ Done downloading all PDFs.")

from google.colab import drive
drive.mount('/content/drive')

import shutil

# Copy folder to a new location in your Google Drive
shutil.copytree('irs_all_pdfs', '/content/drive/MyDrive/UTD Coursework/Sem 4/AI Agents project/Tax project/irs_all_pdfs')

import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Folder to save PDFs
os.makedirs("irs_all_pdfs", exist_ok=True)

base_url = "https://www.irs.gov"
start_url = "https://www.irs.gov/downloads/irs-pdf?page={}"

headers = {
    "User-Agent": "Mozilla/5.0"
}

for page_num in range(58):  # Pages 0 to 57
    print(f"\nüîç Scraping page {page_num}...")
    url = start_url.format(page_num)

    try:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
    except Exception as e:
        print(f"‚ùå Failed to fetch page {page_num}: {e}")
        continue

    soup = BeautifulSoup(res.text, "html.parser")

    # Find all <a> tags with href ending in .pdf
    links = soup.find_all("a", href=True)

print(links[0])

